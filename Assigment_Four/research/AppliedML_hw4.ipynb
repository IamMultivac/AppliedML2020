{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno\n\nimport matplotlib.pyplot as plt\nimport plotly_express as px\nplt.style.use(\"seaborn-whitegrid\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport time\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import PolynomialFeatures\nimport re\nimport string\nimport spacy\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import MaxAbsScaler\n\n# Word vectors documentation\n# https://medium.com/analytics-vidhya/theory-behind-word-embeddings-in-word2vec-858b9350870b\n# https://medium.com/analytics-vidhya/glove-theory-and-python-implementation-b706aea28ac1\n\n# How to add GloVe vectors\n# https://www.kaggle.com/general/35746\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(texto, lemmatize = True):\n    texto = \" \".join([w.lower() for w in texto.split()])\n    texto = re.sub(r\"[^A-Za-z0-9]\", \" \", texto)\n    if lemmatize:\n        nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n        doc = nlp(texto)\n        texto = \" \".join([token.lemma_ for token in doc if token.lemma_ != \"-PRON-\"])\n    \n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_bottom_k(frame, feature_name = None, feature_value = None, agg_func = np.mean,  top_k = 25):\n    \"\"\"\n    Returns a colum from the dataframe cut into 3 categories as belongs to top_k, belongs to bottom k and others\n    Returns pd.Series\n    \"\"\"\n    \n    top_frame = frame.groupby(feature_name).agg({feature_value:agg_func}).nlargest(top_k, columns = feature_value).index.tolist()\n    bot_frame = frame.groupby(feature_name).agg({feature_value:agg_func}).nsmallest(top_k, columns = feature_value).index.tolist()\n    other_frame = list(set(frame[feature_name].unique()) - (set(top_frame).union(set(bot_frame))))\n    \n    #print(other_frame)\n    \n    val_top = [\"top_k\"]* top_k\n    val_bot = [\"bot_k\"]* top_k\n    val_other = [\"other\"]*len(other_frame)\n\n    \n    series = frame[feature_name].replace(dict(zip(top_frame,val_top))).replace(dict(zip(bot_frame,val_bot))).replace(dict(zip(other_frame,val_other)))\n    \n    \n    return series\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_model_name(model):\n    \"\"\"\n        Returns a string with the name of a sklearn model\n            model: Sklearn stimator class\n    \"\"\"\n    if isinstance(model, Pipeline):\n        estimator = model.steps[-1][1]\n        name = \"Pipeline_\" + str(estimator)[:str(estimator).find(\"(\")]\n    else: \n        name = str(model)[:str(model).find(\"(\")]\n    return name \n\ndef plot_cv_score(X, y, models_list, cv = 5, scoring = None, refit = True, verbose = True):\n    \"\"\" \n        X: numpy_array/pandas dataframe n_rows, m_features\n        y: numpy_array/pandas dataframe n_rows\n        Plots min, max and avg kfold crosval_score for a list of models\n    \n    \"\"\"\n\n    \n    \n    names, scores, min_score, max_score, mean_score = list(), list(), list(), list(), list()\n\n    for i, model in enumerate(models_list):\n        t0 = time.time()\n        name = _get_model_name(model)\n        names.append(name)\n\n        if refit:\n            model.fit(X, y)\n        \n        score = cross_val_score(model, X, y, cv = cv, scoring = scoring, n_jobs= -1)\n\n        min_score.append(np.min(score))\n        max_score.append(np.max(score))\n        mean_score.append(np.mean(score))\n        scores.append(score)\n        t1 = time.time()\n        \n        if verbose:\n            print(f\"Iteration: {i} done in {round((t1-t0)/60,2)} minutes\")\n            print(f\"Mean score for model: {names[i]}: {mean_score[i]}\")\n        \n            \n    \n    frame_summary = pd.DataFrame({'Min':min_score, 'Average': mean_score, 'Max': max_score,}, index = names).sort_values(by = 'Average')\n\n    frame_scores = pd.DataFrame(np.vstack(scores).T, columns = names) \n\n\n    fig, ax  = plt.subplots(1,2, figsize = (15,7))\n\n    frame_summary.plot.barh(edgecolor = 'black', ax = ax[0], cmap = 'RdYlBu')\n    ax[0].legend(loc = 'best')\n    ax[0].set_xlabel(\"Score\")\n\n    frame_scores.boxplot(ax = ax[1])\n    ax[1].set_title(\"Model scores distribution\")\n    ax[1].set_ylabel(\"Score\")\n    ax[1].tick_params(labelrotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_importances(estimator, X, y, scoring = None, n_repeats = 5, n_jobs = -1):\n    \"\"\"\n    Computes permutation feature importance for a given model\n    \"\"\"\n    pimp = permutation_importance(estimator= estimator, X= X, y = y, n_repeats= n_repeats, n_jobs = n_jobs)\n    \n    df = pd.DataFrame({\"Mean performance decrease\":pimp.importances_mean}, index = X.columns).sort_values(by = \"Mean performance decrease\")\n    \n    fig, ax = plt.subplots(figsize = (10,5))\n    \n    df.plot.barh(ax = ax, edgecolor = \"black\", cmap = \"RdYlBu\")\n    ax.set_title(\"Importances\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 1 Bag of Words and simple Features [50pts]\n#### 1.1 Create a baseline model for predicting wine quality using only non-text features.\n#### 1.2 Create a simple text-based model using a bag-of-words approach and a linear model.\n#### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets.\n##### 1.3 Combine the non-text features and the text features. How does adding those features improve upon just using bag-of-words?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/wine-reviews/winemag-data-130k-v2.csv\")\ndata.drop(\"Unnamed: 0\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"train_test\"] = np.random.choice(a = [\"train\", \"test\"], p = [.7,.3], size = data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train = data[data.train_test == \"train\"]\nXy_test = data[data.train_test == \"test\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(Xy_train, figsize = (12,7));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.hist(figsize = (12,7), edgecolor = \"black\", color = \"darkred\", bins = \"auto\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"country\").price.mean().sort_values().plot.barh(color = \"orange\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average price by country\")\n\nXy_train.boxplot(column = \"price\", by = \"country\", ax = ax[1], rot = 90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train[\"country_winery\"] = Xy_train[\"country\"] + \" | \" + Xy_train[\"winery\"]\nXy_train[\"country_designation\"] = Xy_train[\"country\"] + \" | \" + Xy_train[\"designation\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"country_winery\").price.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average price by winery\")\n\nXy_train.groupby(\"country_winery\").price.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average price by winery\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"country_winery\").points.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average points by winery\")\n\nXy_train.groupby(\"country_winery\").points.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average points by winery\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"designation\").price.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average price by winery\")\n\nXy_train.groupby(\"designation\").price.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average price by winery\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"country_designation\").price.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average price by winery\")\n\nXy_train.groupby(\"country_designation\").price.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average price by winery\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"designation\").points.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average points by designation\")\n\nXy_train.groupby(\"designation\").points.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average points by designation\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (20,7))\nXy_train.groupby(\"variety\").points.mean().nlargest(25).sort_values().plot.barh(color = \"darkred\", edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Average points by variety\")\n\nXy_train.groupby(\"variety\").points.mean().nsmallest(25).sort_values().plot.barh(color = \"darkblue\", edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Average points by variety\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(Xy_train.price, Xy_train.price, alpha = .5, facecolors = \"none\", edgecolor = \"darkred\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Points\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.1 Create a baseline model for predicting wine quality using only non-text features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, StandardScaler\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor, XGBRFRegressor\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import SGDRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train = Xy_train[[\"price\", \"country\", \"points\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\", \"description\"]]\nXy_test = Xy_test[[\"price\", \"country\", \"points\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\", \"description\"]]\n\nX_train = Xy_train.drop([\"points\", \"description\"], axis = 1)\ny_train = Xy_train.points\n\nX_test = Xy_test.drop([\"points\", \"description\"], axis = 1)\ny_test = Xy_test.points","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_prepro = Pipeline([(\"impute\",SimpleImputer(strategy = \"median\")),(\"scale\",StandardScaler()), (\"discretizer\", KBinsDiscretizer(strategy = \"kmeans\", n_bins=7))])\ncat_prepro = Pipeline([(\"impute\",SimpleImputer(fill_value= \"missing\", strategy = \"constant\")),(\"encoding\",OneHotEncoder(handle_unknown = \"ignore\"))])\n\npreprocessor = make_column_transformer((cat_prepro, make_column_selector(dtype_include = \"object\")), (cont_prepro, make_column_selector(dtype_exclude = \"object\")))\n\npipe_XGB = Pipeline([(\"preprocessing\", preprocessor), (\"model\", XGBRegressor())]).fit(X_train, y_train)\npipe_RF = Pipeline([(\"preprocessing\", preprocessor), (\"model\", XGBRFRegressor())]).fit(X_train, y_train)\npipe_Linear = Pipeline([(\"preprocessing\", preprocessor), (\"model\", SGDRegressor())]).fit(X_train, y_train)\n\nmodels = [pipe_XGB, pipe_RF, pipe_Linear]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train, y_train, models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_XGB).fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importances(pipe_XGB, X_train, y_train, scoring = None, n_repeats = 5, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.2 Create a simple text-based model using a bag-of-words approach and a linear model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = Xy_train[[\"description\"]]\ny_train = Xy_train.points\n\nX_test = Xy_train[[\"description\"]]\ny_test = Xy_train.points\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_vect = Pipeline([(\"bow\",CountVectorizer(min_df = 2, stop_words = ENGLISH_STOP_WORDS))])\n\npreprocessor = make_column_transformer((text_vect, \"description\"))\n\npipe_XGB = Pipeline([(\"preprocessing\", preprocessor), (\"model\", XGBRegressor())]).fit(X_train, y_train)\npipe_RF = Pipeline([(\"preprocessing\", preprocessor), (\"model\", XGBRFRegressor())]).fit(X_train, y_train)\npipe_Linear = Pipeline([(\"preprocessing\", preprocessor),(\"scaler\", MaxAbsScaler()), (\"model\", SGDRegressor())]).fit(X_train, y_train)\n\nmodels = [pipe_Linear, pipe_XGB, pipe_RF ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train, y_train, models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_Linear).fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (12,7))\npd.DataFrame({\"Importances\":pipe_Linear[\"model\"].coef_}, index = pipe_Linear[\"preprocessing\"].transformers_[0][1][0].get_feature_names()).sort_values(by = \"Importances\",\nascending = False).nlargest(25, columns = \"Importances\").plot.barh(edgecolor = \"black\", color = \"darkgreen\", ax = ax[0])\nax[0].set_title(\"top positive feature importances gain Linear transformer\")\n\npd.DataFrame({\"Importances\":pipe_Linear[\"model\"].coef_}, index = pipe_Linear[\"preprocessing\"].transformers_[0][1][0].get_feature_names()).sort_values(by = \"Importances\",\nascending = False).nsmallest(25, columns = \"Importances\").plot.barh(edgecolor = \"black\", color = \"darkred\", ax = ax[1])\nax[1].set_title(\"top negative feature importances Linear transformer\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_vect = Pipeline([(\"bow\",TfidfVectorizer(min_df = 2,max_features = 500, stop_words = ENGLISH_STOP_WORDS, ngram_range = (1,2)))])\n\npreprocessor = make_column_transformer((text_vect, \"description\"))\n\npipe_XGB = Pipeline([(\"preprocessing\", preprocessor), (\"model\", XGBRegressor())]).fit(X_train, y_train)\npipe_Linear = Pipeline([(\"preprocessing\", preprocessor),(\"scaler\", MaxAbsScaler()), (\"model\", SGDRegressor(max_iter = 10000, penalty = \"l1\"))]).fit(X_train, y_train)\n\nmodels = [pipe_Linear, pipe_XGB]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train, y_train, models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_XGB).fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using all features\nXy_train = Xy_train[[\"price\", \"country\", \"points\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\", \"description\"]]\nXy_test = Xy_test[[\"price\", \"country\", \"points\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\", \"description\"]]\n\nX_train = Xy_train.drop([\"points\"], axis = 1)\ny_train = Xy_train.points\n\nX_test = Xy_test.drop([\"points\"], axis = 1)\ny_test = Xy_test.points","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_feat = [\"price\"]\ncat_feat = [\"country\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\"]\ntext_feat = \"description\"\n\ncont_preporcesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"median\")),(\"discretizer\", KBinsDiscretizer(strategy = \"kmeans\", n_bins=7))])\ncat_preprocesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"constant\", fill_value = \"missing\")), (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))])\ntext_preprocesor = Pipeline([(\"vectorizer\", CountVectorizer(min_df = 2, stop_words = ENGLISH_STOP_WORDS))])\n\npreprocesor = make_column_transformer((cont_preporcesor, cont_feat), (cat_preprocesor, cat_feat), (text_preprocesor, text_feat))\n\npipe_XGB = Pipeline([(\"preprocessing\", preprocesor), (\"model\", XGBRegressor())]).fit(X_train, y_train)\npipe_Linear = Pipeline([(\"preprocessing\", preprocesor), (\"model\", SGDRegressor(max_iter = 10000, penalty = \"l1\"))]).fit(X_train, y_train)\n\nmodels = [pipe_XGB, pipe_Linear]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train, y_train, models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_Linear).fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hiperparameter optimization\n\ncont_feat = [\"price\"]\ncat_feat = [\"country\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\"]\ntext_feat = \"description\"\n\ncont_preporcesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"median\")),(\"discretizer\", KBinsDiscretizer(strategy = \"kmeans\", n_bins=7))])\ncat_preprocesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"constant\", fill_value = \"missing\")), (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))])\ntext_preprocesor = Pipeline([(\"vectorizer\", CountVectorizer(min_df = 2, stop_words = ENGLISH_STOP_WORDS))])\n\npreprocesor = make_column_transformer((cont_preporcesor, cont_feat), (cat_preprocesor, cat_feat), (text_preprocesor, text_feat))\n\n\npipe_Linear = Pipeline([(\"preprocessing\", preprocesor), (\"model\", SGDRegressor(max_iter = 10000, penalty = \"l1\"))])\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {\"preprocessing__pipeline-3__vectorizer__min_df\":[1,2,5,7,10],\n              \"preprocessing__pipeline-1__discretizer__strategy\":[\"kmeans\", \"quantile\"],\n              \"model__penalty\":[\"l1\", \"l2\"]    \n}\n\npipe = RandomizedSearchCV(estimator= pipe_Linear, param_distributions= param_grid, cv= 5, scoring= None, random_state= 1990,n_jobs= -1, verbose = True).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train, y_train, [pipe.best_estimator_],refit = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe.best_estimator_).fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.model_selection import LearningCurve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer =LearningCurve(pipe.best_estimator_).fit(X_train, y_train)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 2 Word Vectors [50pts]\nUse a pretrained word-embedding (word2vec, glove or fasttext) for featurization instead of the\nbag-of-words model. Does this improve classification? How about combining the embedded\nwords with the BoW model?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def embeddings_dataframe(X_train, X_test,text_column = None, embeddings_prefix = \"emb_\"):\n    train_text = X_train[text_column]\n    test_text = X_test[text_column]\n    all_text = pd.concat([train_text, test_text])\n\n    print(\"Checkpoint1 - Data Read Complete\")\n\n    embeddings_index = {}\n    f = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', encoding=\"utf8\")\n    for line in tqdm(f):\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        except ValueError:\n            pass\n    f.close()\n    print('Found %s word vectors.' % len(embeddings_index))\n    # this function creates a normalized vector for the whole sentence\n    def sent2vec(s):\n        words = str(s).lower()\n        words = word_tokenize(words)\n        words = [w for w in words if not w in ENGLISH_STOP_WORDS]\n        words = [w for w in words if w.isalpha()]\n        M = []\n        for w in words:\n            try:\n                M.append(embeddings_index[w])\n            except:\n                continue\n        M = np.array(M)\n        v = M.sum(axis=0)\n        if type(v) != np.ndarray:\n            return np.zeros(100)\n        return v / np.sqrt((v ** 2).sum())\n\n    # create sentence vectors using the above function for training and validation set\n    xtrain_glove = [sent2vec(x) for x in tqdm(train_text)]\n    xtest_glove = [sent2vec(x) for x in tqdm(test_text)]\n\n    print('Checkpoint2 -Normalized Vector for Sentences are created')\n\n    xtrain_glove = np.array(xtrain_glove)\n    xtest_glove = np.array(xtest_glove)\n    \n    xtrain_glove = pd.DataFrame(xtrain_glove, columns = [embeddings_prefix + str(i) for i in range(xtrain_glove.shape[1])])\n    xtest_glove = pd.DataFrame(xtest_glove, columns = [embeddings_prefix + str(i) for i in range(xtest_glove.shape[1])])\n    \n    X_train = pd.concat([X_train.reset_index(drop = True), xtrain_glove], axis = 1)\n    X_test = pd.concat([X_test.reset_index(drop = True), xtest_glove], axis = 1)\n    \n    return (X_train, X_test)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_emb, X_test_emb = embeddings_dataframe(X_train, X_test, \"description\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import make_column_selector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_feat = [\"price\"]\n# emb_feat = [x for x in X_train_emb.columns if \"emb\" in x]\ncat_feat = [\"country\", \"winery\", \"region_1\", \"designation\", \"variety\", \"province\", \"taster_name\"]\ntext_feat = \"description\"\n\ncont_preporcesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"median\")),(\"discretizer\", KBinsDiscretizer(strategy = \"kmeans\", n_bins=7))])\nemb_preprocesor = Pipeline([(\"scaler\", MaxAbsScaler())])\ncat_preprocesor = Pipeline([(\"imputer\",SimpleImputer(strategy = \"constant\", fill_value = \"missing\")), (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))])\ntext_preprocesor = Pipeline([(\"vectorizer\", CountVectorizer(min_df = 2, stop_words = ENGLISH_STOP_WORDS))])\n\npreprocesor = make_column_transformer((cont_preporcesor, cont_feat), (cat_preprocesor, cat_feat), (text_preprocesor, text_feat), remainder = \"passthrough\")\n\npipe_XGB = Pipeline([(\"preprocessing\", preprocesor), (\"model\", XGBRegressor())]).fit(X_train, y_train).fit(X_train_emb, y_train)\npipe_Linear = Pipeline([(\"preprocessing\", preprocesor), (\"model\", SGDRegressor(max_iter = 10000, penalty = \"l1\"))]).fit(X_train_emb, y_train)\n\nmodels = [pipe_XGB, pipe_Linear]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(X_train_emb, y_train,models, refit = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 3 Transformers (bonus / optional) [50pts]\nFine-tune a BERT model on the text data alone using the transformers library.\nHow does this model compare to a BoW model, and how does it compare to a model using all\nfeatures?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://medium.com/swlh/transformer-fine-tuning-for-sentiment-analysis-c000da034bb5\n# https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! pip install torch==1.1.0 pytorch-transformers pytorch-ignite","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = list(set(X_train..tolist()))\n\n# labels to integers mapping\nlabel2int = {label: i for i, label in enumerate(labels)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, random_split, DataLoader\nimport numpy as np\nimport warnings\nfrom tqdm import tqdm_notebook as tqdm\nfrom typing import Tuple\n\nNUM_MAX_POSITIONS = 256\nBATCH_SIZE = 32\n\nclass TextProcessor: \n    # special tokens for classification and padding\n    CLS = '[CLS]'\n    PAD = '[PAD]'\n    \n    def __init__(self, tokenizer, label2id: dict, num_max_positions:int=512):\n        self.tokenizer=tokenizer\n        self.label2id = label2id\n        self.num_labels = len(label2id)\n        self.num_max_positions = num_max_positions     \n    \n    def process_example(self, example: Tuple[str, str]):\n        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n        assert len(example) == 2\n        label, text = example[0], example[1]\n        assert isinstance(text, str)\n        tokens = self.tokenizer.tokenize(text)\n\n        # truncate if too long\n        if len(tokens) >= self.num_max_positions:\n            tokens = tokens[:self.num_max_positions-1] \n            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n        # pad if too short\n        else:\n            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n        \n        return ids, self.label2id[label]\n\n# download the 'bert-base-cased' tokenizer\nfrom pytorch_transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n\n# initialize a TextProcessor\nprocessor = TextProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}