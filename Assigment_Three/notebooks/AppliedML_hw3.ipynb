{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, Ridge, SGDRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import make_column_transformer, TransformedTargetRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor\nfrom xgboost import XGBRFRegressor, XGBRegressor\n\n\nplt.style.use(\"seaborn-whitegrid\")\n\npd.set_option(\"display.max_columns\", None)\n\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport time\n\nfrom yellowbrick.regressor import ResidualsPlot\nfrom yellowbrick.model_selection import LearningCurve\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Ok\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_model_name(model):\n    \"\"\"\n        Returns a string with the name of a sklearn model\n            model: Sklearn stimator class\n    \"\"\"\n    if isinstance(model, Pipeline):\n        estimator = model.steps[-1][1]\n        name = \"Pipeline_\" + str(estimator)[:str(estimator).find(\"(\")]\n    else: \n        name = str(model)[:str(model).find(\"(\")]\n    return name \n\ndef plot_cv_score(X, y, models_list, cv = 5, scoring = None, refit = True, verbose = True):\n    \"\"\" \n        X: numpy_array/pandas dataframe n_rows, m_features\n        y: numpy_array/pandas dataframe n_rows\n        Plots min, max and avg kfold crosval_score for a list of models\n    \n    \"\"\"\n\n    \n    \n    names, scores, min_score, max_score, mean_score = list(), list(), list(), list(), list()\n\n    for i, model in enumerate(models_list):\n        t0 = time.time()\n        name = _get_model_name(model)\n        names.append(name)\n\n        if refit:\n            model.fit(X, y)\n        \n        score = cross_val_score(model, X, y, cv = cv, scoring = scoring, n_jobs= -1)\n\n        min_score.append(np.min(score))\n        max_score.append(np.max(score))\n        mean_score.append(np.mean(score))\n        scores.append(score)\n        t1 = time.time()\n        \n        if verbose:\n            print(f\"Iteration: {i} done in {round((t1-t0)/60,2)} minutes\")\n            print(f\"Mean score for model: {names[i]}: {mean_score[i]}\")\n        \n            \n    \n    frame_summary = pd.DataFrame({'Min':min_score, 'Average': mean_score, 'Max': max_score,}, index = names).sort_values(by = 'Average')\n\n    frame_scores = pd.DataFrame(np.vstack(scores).T, columns = names) \n\n\n    fig, ax  = plt.subplots(1,2, figsize = (15,7))\n\n    frame_summary.plot.barh(edgecolor = 'black', ax = ax[0], cmap = 'RdYlBu')\n    ax[0].legend(loc = 'best')\n    ax[0].set_xlabel(\"Score\")\n\n    frame_scores.boxplot(ax = ax[1])\n    ax[1].set_title(\"Model scores distribution\")\n    ax[1].set_ylabel(\"Score\")\n    ax[1].tick_params(labelrotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_importances(estimator, X, y, scoring = None, n_repeats = 5, n_jobs = -1):\n    \"\"\"\n    Computes permutation feature importance for a given model\n    \"\"\"\n    pimp = permutation_importance(estimator= estimator, X= X, y = y, n_repeats= n_repeats, n_jobs = n_jobs)\n    \n    df = pd.DataFrame({\"Mean performance decrease\":pimp.importances_mean}, index = X.columns).sort_values(by = \"Mean performance decrease\")\n    \n    fig, ax = plt.subplots(figsize = (10,5))\n    \n    df.plot.barh(ax = ax, edgecolor = \"black\", cmap = \"RdYlBu\")\n    ax.set_title(\"Importances\")\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal of this homework is to provide a realistic setting for a machine learning task.\nTherefore instructions will not specify the exact steps to carry out. Instead, it is part of the\nassignment to identify promising features, models and preprocessing methods and apply them\nas appropriate.\n**The overall goal is to predict the price of a used vehicle on craigslist**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Task 1 Identify Features\nAssemble a dataset consisting of features and target (for example in a dataframe or in two\narrays X and y). What features are relevant for the prediction task?\nAre there any features that should be excluded because they leak the target information?\nShow visualizations or statistics to support your selection.\nYou are not required to use the description column, but you can try to come up with relevant\nfeatures using it. Please don’t use bag-of-word approaches for now as we’ll discuss these later\nin the class.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/craigslist-carstrucks-data/vehicles.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = data.sample(frac = .5, random_state = 1990)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[(data.price >= 549) & (data.year > 0)].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to : https://www.kaggle.com/aantonova/some-new-risk-and-clusters-features\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = [\"id\", \"url\", \"region_url\", \"vin\", \"image_url\", \"description\", \"county\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"train_test\"] = np.random.choice(a = [\"train\", \"test\"], p = [.70, .30], size = data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train = data[data.train_test == \"train\"].drop(columns_to_drop, axis = 1)\nXy_test = data[data.train_test == \"test\"].drop(columns_to_drop, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.price.describe(percentiles = np.arange(0,1,.01))[4:].plot(marker = \"x\", color = \"darkgreen\")\nplt.title(\"Precentiles plot for price\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.odometer.describe(percentiles = np.arange(0,1,.01))[4:].plot(marker = \"x\", color = \"darkgreen\")\nplt.title(\"Precentiles plot for odometer\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers in target but only using as reference the values on the train set\nXy_train = Xy_train[Xy_train.price <= np.quantile(Xy_train.price,.99)].reset_index(drop = True)\nXy_test = Xy_test[Xy_test.price <= np.quantile(Xy_train.price,.99)].reset_index(drop = True)\n\n# Remove outliers in target but only using as reference the values on the train set\nXy_train = Xy_train[Xy_train.odometer <= np.quantile(Xy_train.odometer.fillna(0),.99)].reset_index(drop = True)\nXy_test = Xy_test[Xy_test.odometer <= np.quantile(Xy_train.odometer.fillna(0),.99)].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.groupby(\"manufacturer\").region.count().sort_values().plot.bar(figsize = (17,7),\n                                                           alpha = .5,\n                                                           color = \"darkred\",\n                                                           edgecolor = \"black\")\nplt.tight_layout()\nplt.title(\"Sold cars by manufacturer\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.groupby(\"year\").region.count().to_frame().plot.bar(figsize = (17,7),\n                                                           alpha = .5,\n                                                           color = \"grey\",\n                                                           edgecolor = \"black\")\nplt.tight_layout()\nplt.title(\"Sold cars by model year\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.groupby(\"year\").price.mean().to_frame().plot.bar(figsize = (17,7),\n                                                           alpha = .5,\n                                                           color = \"darkred\",\n                                                           edgecolor = \"black\")\nplt.tight_layout()\nplt.title(\"Mean price by model year\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group unreesentative groups of manufacturer and year\nlux = [\"ferrari\", \"porche\", \"aston-martin\", \"land rover\", \"datsun\", \"alfa-romeo\", \"tesla\", \"harley-davidson\", \"morgan\"]\nkey = [\"rare\"]*9\ndict_manu = dict(zip(lux,key))\nXy_train.manufacturer.replace(dict_manu, inplace = True)\n\n# Group unreesentative groups of manufacturer and year\nXy_train[\"year\"] = np.where(Xy_train.year < 1960, 1900, Xy_train.year)\n\nXy_test.manufacturer.replace(dict_manu, inplace = True)\n\n# Group unreesentative groups of manufacturer and year\nXy_test[\"year\"] = np.where(Xy_test.year < 1960, 1900, Xy_test.year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nXy_train.groupby(\"year\").region.count().to_frame().plot.bar(figsize = (30,7),\n                                                           alpha = .5,\n                                                           color = \"grey\",\n                                                           edgecolor = \"black\", ax = ax[0])\n\nax[0].set_title(\"Sold cars by model year\")\n\n\nXy_train.groupby(\"manufacturer\").region.count().sort_values().plot.bar(figsize = (30,7),\n                                                           alpha = .5,\n                                                           color = \"darkred\",\n                                                           edgecolor = \"black\", ax = ax[1])\n\nax[1].set_title(\"Sold cars by manufacturer\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (12,7))\nXy_train.groupby(\"region\").agg({\"price\":np.mean}).sort_values(by = \"price\").head(10).plot.barh(edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Top 10 Regions with the lowest avg prices\")\n\nXy_train.groupby(\"region\").agg({\"price\":np.mean}).sort_values(by = \"price\").tail(10).plot.barh(color = \"orange\",edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Top 10 Regions with the highest avg prices\")\nplt.tight_layout();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (12,7))\nXy_train.groupby(\"state\").agg({\"price\":np.mean}).sort_values(by = \"price\").head(10).plot.barh(edgecolor = \"black\", ax = ax[0])\nax[0].set_title(\"Top 10 States with the lowest avg prices\")\n\nXy_train.groupby(\"state\").agg({\"price\":np.mean}).sort_values(by = \"price\").tail(10).plot.barh(color = \"orange\",edgecolor = \"black\", ax = ax[1])\nax[1].set_title(\"Top 10 States with the highest avg prices\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.groupby(\"manufacturer\").agg({\"price\":np.mean}).sort_values(by = \"price\").plot.barh(figsize = (12,7),color = \"orange\",edgecolor = \"black\")\nplt.title(\"Average price by manufacturer\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy_train.groupby(\"condition\").agg({\"price\":np.mean}).sort_values(by = \"price\").plot.barh(figsize = (12,7),\n                                                                                         color = \"darkgreen\",edgecolor = \"black\")\nplt.title(\"Average price by condition\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.log1p(Xy_train.odometer), np.log1p(Xy_train.price), alpha = .2)\nplt.title(\"Odometer vs price\")\nplt.xlabel(\"Log odometer\")\nplt.ylabel(\"Log price\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 2 Preprocessing and Baseline Model\nCreate a simple minimum viable model by doing an initial selection of features, doing\nappropriate preprocessing and cross-validating a linear model. Feel free to exclude features or\ndo simplified preprocessing for this task. As mentioned before, you don’t need to validate the\nmodel on the whole dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = Xy_train.drop([\"price\", \"lat\", \"long\", \"train_test\"], axis = 1).reset_index(drop = True)\ny_train = Xy_train[\"price\"]\n\nX_test = Xy_test.drop([\"price\", \"lat\", \"long\", \"train_test\"], axis = 1).reset_index(drop = True)\ny_test = Xy_test[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = X_train.select_dtypes(include = \"object\").columns.tolist()\ncont_feat = X_train.select_dtypes(exclude = \"object\").columns.tolist()\n\nnumeric_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy= \"median\")),\n                                (\"binning\", KBinsDiscretizer(encode = \"onehot-dense\", strategy= \"kmeans\"))])\n\nobj_transformer = Pipeline([(\"Imputer\", SimpleImputer(strategy= \"constant\", fill_value = \"missing\")),\n                                (\"targenc\", OneHotEncoder(handle_unknown = \"ignore\"))])\n\nscaler = make_column_transformer((numeric_transformer, [\"odometer\"]), (obj_transformer, [\"manufacturer\", \"condition\", \"cylinders\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_Lasso = Pipeline([(\"preprocessor\", scaler), (\"model\", Lasso())])\npipe_Ridge = Pipeline([(\"preprocessor\", scaler), (\"model\", Ridge())])\npipe_OLS = Pipeline([(\"preprocessor\", scaler), (\"model\", SGDRegressor(penalty = \"elasticnet\"))])\npipe_SVM = Pipeline([(\"preprocessor\", scaler), (\"model\", LinearSVR())])\n\n\nmodels = [pipe_Lasso, pipe_Ridge, pipe_OLS, pipe_SVM]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(models_list = models,X = X_train[['odometer', 'manufacturer', 'condition', 'cylinders']] ,y = y_train, refit = True, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_OLS)\nvisualizer.fit(X_train[['odometer', 'manufacturer', 'condition', 'cylinders']] ,y = y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear models are giving negative predictions in some cases, it might be due to the non linear functional form as hipotesys of OLS model as presented [here](https://stats.stackexchange.com/questions/145383/getting-negative-predicted-values-after-linear-regression)\n\nSo I'm going to use a tranformation for the targe variable (Price)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_Lasso = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = Lasso(), func = np.log1p, inverse_func = np.expm1))])\npipe_Ridge = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = Ridge(), func = np.log1p, inverse_func = np.expm1))])\npipe_OLS = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = SGDRegressor(penalty = \"elasticnet\"), func = np.log1p, inverse_func = np.expm1))])\npipe_SVM = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = LinearSVR(), func = np.log1p, inverse_func = np.expm1))])\n\n\nmodels = [pipe_Lasso, pipe_Ridge, pipe_OLS, pipe_SVM]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(models_list = models,X = X_train[['odometer', 'manufacturer', 'condition', 'cylinders']] ,y = y_train, refit = True, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_SVM)\nvisualizer.fit(X_train[['odometer', 'manufacturer', 'condition', 'cylinders']] ,y = y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 3 Feature Engineering\nCreate derived features and perform more in-depth preprocessing and data cleaning. Does this\nimprove your model? In particular, think about how to encode categorical variables and\nwhether adding interactions (for example using PolynomialFeatures or manually) might help.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on descriptive statistics I'm going to drop useless variables, also I'm dropping variables with quite much categories that might become aso sparce when encoding\nX_train = Xy_train.drop([\"price\", \"train_test\", \"state\", \"region\", \"model\", \"paint_color\"], axis = 1).reset_index(drop = True)\ny_train = Xy_train[\"price\"]\n\nX_test = Xy_test.drop([\"price\", \"train_test\", \"state\", \"region\", \"model\", \"paint_color\"], axis = 1).reset_index(drop = True)\ny_test = Xy_test[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = X_train.select_dtypes(include = \"object\").columns.tolist()\ncont_feat = X_train.select_dtypes(exclude = \"object\").columns.tolist()\n\nnumeric_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = 0)),\n                                (\"binning\", KBinsDiscretizer(encode = \"onehot-dense\", strategy= \"kmeans\", n_bins = 5))])\n\nobj_transformer = Pipeline([(\"Imputer\", SimpleImputer(strategy= \"constant\", fill_value = \"missing\")),\n                                (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\", ))])\n\nscaler = make_column_transformer((numeric_transformer, cont_feat), (obj_transformer, cat_feat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_Lasso = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = Lasso(), func = np.log1p, inverse_func = np.expm1))])\npipe_Ridge = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = Ridge(), func = np.log1p, inverse_func = np.expm1))])\npipe_OLS = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = SGDRegressor(penalty = \"elasticnet\"), func = np.log1p, inverse_func = np.expm1))])\npipe_SVM = Pipeline([(\"preprocessor\", scaler), (\"model\", TransformedTargetRegressor(regressor = LinearSVR(), func = np.log1p, inverse_func = np.expm1))])\n\nmodels = [pipe_Lasso, pipe_Ridge, pipe_OLS, pipe_SVM]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(models_list = models,X = X_train ,y = y_train, refit = True, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importances(pipe_Ridge, X_train, y_train, scoring = None, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 4 Any model\nUse any regression model we discussed (trees, forests, gradient boosting, SVM) to improve\nyour result. You can (and probably should) change your preprocessing and feature engineering\nto be suitable for the model. You are not required to try all of these models. Tune parameters\nas appropriate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = X_train.select_dtypes(include = \"object\").columns.tolist()\ncont_feat = X_train.select_dtypes(exclude = \"object\").columns.tolist()\n\nnumeric_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = 0))])\n\nobj_transformer = Pipeline([(\"Imputer\", SimpleImputer(strategy= \"constant\", fill_value = \"missing\")),\n                                (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))])\n\nscaler = make_column_transformer((numeric_transformer, cont_feat), (obj_transformer, cat_feat))\n\n# pipe_Ensem = Pipeline([(\"scaler\", scaler), (\"model\", BaggingRegressor(base_estimator = XGBRFRegressor(), n_estimators = 50))])\npipe_Xgb = Pipeline([(\"scaler\", scaler), (\"model\", XGBRFRegressor())])\n\n\nmodels = [pipe_Xgb]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(models_list = models,X = X_train ,y = y_train, refit = True, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importances(pipe_Xgb, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_Xgb)\nvisualizer.fit(X_train ,y = y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 5 Feature Selections\nIdentify features that are important for your best model. Which features are most influential,\nand which features could be removed without decrease in performance? Does removing\nirrelevant features make your model better? (This will be discussed in the lecture on 03/04).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop([\"long\", \"lat\", \"title_status\", \"size\", \"transmission\"], axis = 1)\nX_test = X_test.drop([\"long\", \"lat\", \"title_status\", \"size\", \"transmission\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = X_train.select_dtypes(include = \"object\").columns.tolist()\ncont_feat = X_train.select_dtypes(exclude = \"object\").columns.tolist()\n\nnumeric_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = 0))])\n\nobj_transformer = Pipeline([(\"Imputer\", SimpleImputer(strategy= \"constant\", fill_value = \"missing\")),\n                                (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))])\n\nscaler = make_column_transformer((numeric_transformer, cont_feat), (obj_transformer, cat_feat))\n\npipe_EnsemRF = Pipeline([(\"scaler\", scaler), (\"model\", BaggingRegressor(base_estimator = XGBRFRegressor(), n_estimators = 5))])\npipe_XgbRF = Pipeline([(\"scaler\", scaler), (\"model\", XGBRFRegressor())])\npipe_Xgb = Pipeline([(\"scaler\", scaler), (\"model\", XGBRegressor())])\npipe_EnsemBoost = Pipeline([(\"scaler\", scaler), (\"model\", BaggingRegressor(base_estimator = XGBRegressor(), n_estimators = 5))])\n\n\nmodels = [pipe_EnsemRF, pipe_XgbRF,pipe_Xgb,pipe_EnsemBoost ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cv_score(models_list = models,X = X_train ,y = y_train, refit = True, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importances(pipe_Xgb, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(pipe_Xgb)\nvisualizer.fit(X_train ,y = y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is having the undesired behaviuor of predicting negative values for this: [solution](https://github.com/dmlc/xgboost/issues/1581)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Task 6 An explainable model\nCan you create an “explainable” model that is nearly as good as your best model?\nAn explainable model should be small enough to be easily inspected - say a linear model with\nfew enough coefficients that you can reasonably look at all of them, or a tree with a small\nnumber of leaves etc.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instead I'm going to use a model agnostic technique to be able to make my best model \"explainable\"\nfrom sklearn.inspection import plot_partial_dependence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_feat = [\"year\", \"odometer\"]\nplot_partial_dependence(estimator=pipe_EnsemBoost, X =X_train, features =relevant_feat, n_jobs = -1, verbose = 2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}